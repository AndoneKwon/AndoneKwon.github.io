---
layout: post
title: "강화학습 용어정리"
date: 2020-05-06
excerpt: "강화학습 용어정리"
rl: true
comments: true
tag: [강화학습,MDP]
---
# MP?
MP(Markov Property)의 의미는 하나의 타임 스텝만이 영향을 다음 상태에 영향을 끼친다는 것이다. 그보다 더 과거의 상태는 영향을 주지 못한다.

## Markov Chain
Markov Chain은 MP의 확률모델이라고 이야기 할 수 있다. 미래는 현재에만 영향을 받으며 이것은 과거에는 영향을 받지 않는다는 것인데 그러므로 미래는 과거에 독립이다.

또한 Markov Chain은 MP를 따른다. 예를들자면 날씨를 예측할때에 현재의 날씨 상태만을 이용하여 미래의 날씨를 예측할 뿐 현재보다 과거는 신경쓰지 않는 말이다.

MDP(Markov Decision Process)는 Markov Chain의 확장판이라고 할 수 있다.

## MDP?
이것은 decision making(의사결정)을 할 때 수학적 프레임워크를 모델링 해주는 것이라 할 수 있으며 MP를 따르는 decision making process이다.

대부분의 강화학습은 MDP로 모델링 할 수 있다. 강화학습은 특정 Action(행동)을 했을때 최대의 Reward(보상)을 받는 Policy(정책)을 익히는 것인데 어떤 state에서 어떤 행동을 할지를 고르는 것 자체가 decision making이고 그러다보니 MDP가 수학적인 모델링을 제공하기 때문에 MDP를 이용하여 강화학습을 적용하는 경우가 많다.

MDP에서 나오는 용어가 있다.

- States : Agent가 무언가를 하는 환경을 표현하는 state의 집합이다. 예를들어 미로찾기를 했을때 갈 수 있는 경로의 수라고 이야기 할 수 있다.
- Actions : Agent가 할 수 있는 행동들의 집합이라고 할 수 있다. Action을 통해서 transtion(state에서 state로 이동하는 것)을 발생시키며 반드시 transtion이 일어나기 위해선 Action이 필요하다.
- Transtion Probability : Action을 했을때에 특정 state로 갈 확률을 의미한다. 이게 정의 되어 있다는 것은 강화학습을 적용 하는 것에 대해서 모델링이 완벽하다는 의미이다. 이미 알고 있는 환경이라는 의미이다.
- Reward Probability : 의사 결정을 할 때에 이 행동이 좋은건지 안좋은 건지에 대한 Reward가 필요함. 특정 Action을 해서 특정 state에서 state로 옮겨갔을때에 대한 Reward의 기대값을 의미한다.
- Discount factor : Future reward와 immediate reward 중에서 어떤 것을 더 중요하게 여길것인가에 대한 factor 값이다. 

## Episodic Tasks VS Continuous Tasks
Episodic Task는 끝이 존재하는 Task를 의미한다. 예를들어 미로찾기의 경우는 벽에 막혀 실패하는 경우와 final states에 도착하는경우 두가지가 있다.

반면 Continuous Tasks는 끝이 존재하지 않는다. 예를들어 개인비서 로봇을 강화학습을 통해서 학습시킨다고 하면 개인비서 로봇은 끝이 존재하지 않는다.

하지만 여기서 Continuous는 Return을 계산하는 것에 매우 큰 문제점이 발생하는데 Final state(terminal state)가 존재하지 않기 때문에 계속 Reward를 더하다 보면 무한대가 되어버린다.

그래서 여기서 도입하는 개념이 위 용어에서 설명한 Discount factor이다. 기존에는 단순히 Reward를 더하면 됐지만 time step이 늘어날때 마다 Discount factor의 제곱을 늘려준다. 또한 이 값은 [0,1]의 값을 가진다.

그러므로 이 값은 time step이 늘어나면 늘어날수록 점차 0에 수렴하게 되어 무한대가 되지 않게된다.

Episodic Tasks또한 특정 time step을 넘어가게 된다면 0이기 때문에 Continuous Tasks와 동일하게 표현이 가능하다. 